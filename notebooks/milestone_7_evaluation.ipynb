{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d573d3ca",
   "metadata": {},
   "source": [
    "# Milestone 7: Automated Evaluation Pipeline\n",
    "\n",
    "## LLM-as-a-Judge Pattern for Agent Quality Assessment\n",
    "\n",
    "This notebook implements an automated evaluation pipeline for the FinGuard IntelliAgent using the **LLM-as-a-Judge** pattern.\n",
    "\n",
    "### ADK Concepts Demonstrated:\n",
    "\n",
    "1. **Golden Dataset** (Prototype to Production p.12):\n",
    "   - Curated test cases with expected outputs\n",
    "   - Covers all tools and edge cases\n",
    "\n",
    "2. **LLM-as-a-Judge** (Intro to Agents p.29):\n",
    "   - Uses Gemini to grade probabilistic outputs\n",
    "   - Structured evaluation criteria\n",
    "\n",
    "3. **Behavioral Evaluation** (Prototype to Production p.12):\n",
    "   - Assesses the Trajectory (tool selection)\n",
    "   - Not just the final answer\n",
    "\n",
    "4. **Key Metrics**:\n",
    "   - Tool Selection Accuracy\n",
    "   - Goal Completion Rate\n",
    "   - Idempotency Compliance\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Alfred Munga  \n",
    "**Date**: November 18, 2025  \n",
    "**Project**: FinGuard IntelliAgent ADK Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c248c",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56da6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Import agent components\n",
    "from agent.orchestrator import FinGuardIntelliAgent\n",
    "from agent.evaluator import AgentEvaluator, EvaluationResult\n",
    "from backend.utils.logger import AgentLogger\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"üìÖ Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd51d32e",
   "metadata": {},
   "source": [
    "## 2. Load Golden Dataset\n",
    "\n",
    "The golden dataset contains **10 test cases** covering:\n",
    "- SMS parsing (3 cases)\n",
    "- Invoice retrieval (2 cases)\n",
    "- Payment actions (3 cases)\n",
    "- Financial insights (2 cases)\n",
    "\n",
    "Each test case includes:\n",
    "- `query`: User's input\n",
    "- `expected_tool`: Tool that should be called\n",
    "- `criteria`: Success criteria for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f519c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load golden dataset\n",
    "dataset_path = '../data/evaluation/golden_dataset.json'\n",
    "with open(dataset_path, 'r') as f:\n",
    "    golden_dataset = json.load(f)\n",
    "\n",
    "print(f\"üìä Loaded {len(golden_dataset)} test cases\\n\")\n",
    "\n",
    "# Display test case summary\n",
    "categories = {}\n",
    "for test in golden_dataset:\n",
    "    cat = test['category']\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "print(\"Test Case Breakdown:\")\n",
    "for cat, count in categories.items():\n",
    "    print(f\"  - {cat}: {count} cases\")\n",
    "\n",
    "# Show sample test case\n",
    "print(\"\\nüìã Sample Test Case:\")\n",
    "sample = golden_dataset[0]\n",
    "print(json.dumps(sample, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa60baa",
   "metadata": {},
   "source": [
    "## 3. Initialize Agent and Evaluator\n",
    "\n",
    "We initialize:\n",
    "1. **FinGuardIntelliAgent**: The agent under test\n",
    "2. **AgentEvaluator**: The LLM judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "agent = FinGuardIntelliAgent(api_key=api_key)\n",
    "print(\"‚úÖ FinGuardIntelliAgent initialized\")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = AgentEvaluator(api_key=api_key)\n",
    "print(\"‚úÖ AgentEvaluator (Judge) initialized\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to run evaluation pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91be6f0",
   "metadata": {},
   "source": [
    "## 4. Run Batch Evaluation\n",
    "\n",
    "**Evaluation Process**:\n",
    "1. For each test case:\n",
    "   - Run the agent with the query\n",
    "   - Capture execution traces\n",
    "   - Record tools called\n",
    "2. Pass results to LLM Judge\n",
    "3. Calculate aggregate metrics\n",
    "\n",
    "‚ö†Ô∏è **Note**: This may take 5-10 minutes due to API rate limits (10 req/min for free tier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for agent execution results\n",
    "agent_results = []\n",
    "\n",
    "print(\"üîÑ Running agent on all test cases...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, test_case in enumerate(golden_dataset, 1):\n",
    "    test_id = test_case['test_id']\n",
    "    query = test_case['query']\n",
    "    \n",
    "    print(f\"\\n[{i}/{len(golden_dataset)}] Test Case: {test_id}\")\n",
    "    print(f\"Query: {query[:60]}...\" if len(query) > 60 else f\"Query: {query}\")\n",
    "    print(f\"Expected Tool: {test_case['expected_tool']}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run agent\n",
    "        result = agent.run(\n",
    "            user_query=query,\n",
    "            user_id=\"eval_tester\"\n",
    "        )\n",
    "        \n",
    "        execution_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        \n",
    "        # Extract tools called from trace logs\n",
    "        trace_logger = result.get('trace_logger')\n",
    "        trace_logs = trace_logger.logs if trace_logger else []\n",
    "        \n",
    "        tools_called = [\n",
    "            log.get('tool_name') \n",
    "            for log in trace_logs \n",
    "            if log.get('step_type') == 'ACT' and log.get('tool_name')\n",
    "        ]\n",
    "        \n",
    "        agent_results.append({\n",
    "            'response': result.get('response', 'No response'),\n",
    "            'tools_called': tools_called,\n",
    "            'trace_logs': trace_logs,\n",
    "            'execution_time_ms': execution_time\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Tools Called: {', '.join(tools_called) if tools_called else 'None'}\")\n",
    "        print(f\"‚è±Ô∏è  Execution Time: {execution_time:.0f}ms\")\n",
    "        \n",
    "        # Rate limiting: Wait between calls to avoid hitting API limits\n",
    "        if i < len(golden_dataset):\n",
    "            print(\"‚è≥ Waiting 10s to avoid rate limits...\")\n",
    "            time.sleep(10)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)[:100]}\")\n",
    "        agent_results.append({\n",
    "            'response': f\"Error: {str(e)}\",\n",
    "            'tools_called': [],\n",
    "            'trace_logs': [],\n",
    "            'execution_time_ms': 0\n",
    "        })\n",
    "        \n",
    "        # Wait longer after errors\n",
    "        if \"429\" in str(e) or \"quota\" in str(e).lower():\n",
    "            print(\"‚ö†Ô∏è Rate limit hit. Waiting 60s...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\n‚úÖ Agent execution complete: {len(agent_results)}/{len(golden_dataset)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c35fd0",
   "metadata": {},
   "source": [
    "## 5. LLM-as-a-Judge Evaluation\n",
    "\n",
    "Now we pass all agent outputs to the **Judge** (Gemini) for evaluation.\n",
    "\n",
    "The judge grades based on:\n",
    "- **Tool Selection**: Did it call the right tool?\n",
    "- **Goal Achievement**: Was the task completed?\n",
    "- **Idempotency**: Were safety checks performed?\n",
    "- **Response Quality**: Is the answer correct and professional?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öñÔ∏è Starting LLM-as-a-Judge evaluation...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Batch evaluate all results\n",
    "evaluations = evaluator.batch_evaluate(\n",
    "    test_cases=golden_dataset,\n",
    "    agent_results=agent_results\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\n‚úÖ Evaluation complete: {len(evaluations)} test cases graded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380557db",
   "metadata": {},
   "source": [
    "## 6. Display Results\n",
    "\n",
    "Let's examine the evaluation results in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1933d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Test ID': eval_result.test_id,\n",
    "        'Category': eval_result.category,\n",
    "        'Difficulty': eval_result.difficulty,\n",
    "        'Expected Tool': eval_result.expected_tool,\n",
    "        'Tools Called': ', '.join(eval_result.tools_called) or 'None',\n",
    "        'Score': eval_result.judge_evaluation.score,\n",
    "        'Tool Correct': '‚úÖ' if eval_result.judge_evaluation.tool_usage_correct else '‚ùå',\n",
    "        'Goal Achieved': '‚úÖ' if eval_result.judge_evaluation.goal_achieved else '‚ùå',\n",
    "        'Idempotency': '‚úÖ' if eval_result.judge_evaluation.idempotency_respected else '‚ùå',\n",
    "        'Exec Time (ms)': f\"{eval_result.execution_time_ms:.0f}\"\n",
    "    }\n",
    "    for eval_result in evaluations\n",
    "])\n",
    "\n",
    "print(\"üìä Evaluation Results Summary:\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Display detailed results for failed tests\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüîç Detailed Analysis of Failed Tests (Score < 0.7):\\n\")\n",
    "\n",
    "failed_tests = [e for e in evaluations if e.judge_evaluation.score < 0.7]\n",
    "if failed_tests:\n",
    "    for eval_result in failed_tests:\n",
    "        print(f\"Test ID: {eval_result.test_id}\")\n",
    "        print(f\"Query: {eval_result.query}\")\n",
    "        print(f\"Score: {eval_result.judge_evaluation.score:.2f}\")\n",
    "        print(f\"Reasoning: {eval_result.judge_evaluation.reasoning}\")\n",
    "        if eval_result.judge_evaluation.issues:\n",
    "            print(f\"Issues:\")\n",
    "            for issue in eval_result.judge_evaluation.issues:\n",
    "                print(f\"  - {issue}\")\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "else:\n",
    "    print(\"‚úÖ No failed tests! All scores >= 0.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06041763",
   "metadata": {},
   "source": [
    "## 7. Calculate Aggregate Metrics\n",
    "\n",
    "Key metrics as defined in the ADK:\n",
    "- **Tool Selection Accuracy**: % of times correct tool was called (Trajectory evaluation)\n",
    "- **Goal Completion Rate**: % of tests that achieved their goal\n",
    "- **Pass Rate**: % of tests with score >= 0.7\n",
    "- **Average Score**: Overall quality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf64fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "metrics = AgentEvaluator.calculate_metrics(evaluations)\n",
    "\n",
    "print(\"üìä AGGREGATE METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Test Cases: {metrics['total_tests']}\")\n",
    "print(f\"\\nüéØ Tool Selection Accuracy: {metrics['tool_selection_accuracy']:.1%}\")\n",
    "print(f\"   ‚Üí Correct tool called in {int(metrics['tool_selection_accuracy'] * metrics['total_tests'])}/{metrics['total_tests']} cases\")\n",
    "\n",
    "print(f\"\\n‚úÖ Goal Completion Rate: {metrics['goal_completion_rate']:.1%}\")\n",
    "print(f\"   ‚Üí Task completed successfully in {int(metrics['goal_completion_rate'] * metrics['total_tests'])}/{metrics['total_tests']} cases\")\n",
    "\n",
    "print(f\"\\nüìà Pass Rate (Score >= 0.7): {metrics['pass_rate']:.1%}\")\n",
    "print(f\"   ‚Üí {int(metrics['pass_rate'] * metrics['total_tests'])}/{metrics['total_tests']} tests passed\")\n",
    "\n",
    "print(f\"\\n‚≠ê Average Score: {metrics['average_score']:.2f}/1.00\")\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è Idempotency Compliance: {metrics['idempotency_compliance']:.1%}\")\n",
    "print(f\"   ‚Üí Safety checks performed in {int(metrics['idempotency_compliance'] * metrics['total_tests'])}/{metrics['total_tests']} cases\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüìÇ Category Breakdown:\\n\")\n",
    "\n",
    "for category, data in metrics['category_breakdown'].items():\n",
    "    print(f\"{category}:\")\n",
    "    print(f\"  - Tests: {data['count']}\")\n",
    "    print(f\"  - Pass Rate: {data['pass_rate']:.1%}\")\n",
    "    print(f\"  - Avg Score: {data['avg_score']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdedb71",
   "metadata": {},
   "source": [
    "## 8. Save Results to CSV\n",
    "\n",
    "Save results for tracking improvements over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_path = '../data/evaluation/results.csv'\n",
    "AgentEvaluator.save_results(evaluations, output_path)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {output_path}\")\n",
    "print(f\"\\nüìÅ File size: {os.path.getsize(output_path)} bytes\")\n",
    "\n",
    "# Display first few rows\n",
    "import pandas as pd\n",
    "results_csv = pd.read_csv(output_path)\n",
    "print(f\"\\nüìä CSV Preview (first 3 rows):\\n\")\n",
    "print(results_csv.head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d77e7e",
   "metadata": {},
   "source": [
    "## 9. Visualizations\n",
    "\n",
    "Create visualizations for better understanding of evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513591e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('FinGuard IntelliAgent - Evaluation Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Score Distribution\n",
    "scores = [e.judge_evaluation.score for e in evaluations]\n",
    "axes[0, 0].hist(scores, bins=10, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(0.7, color='red', linestyle='--', label='Pass Threshold (0.7)')\n",
    "axes[0, 0].set_xlabel('Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Score Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Category Performance\n",
    "categories = list(metrics['category_breakdown'].keys())\n",
    "category_scores = [metrics['category_breakdown'][cat]['avg_score'] for cat in categories]\n",
    "axes[0, 1].bar(range(len(categories)), category_scores, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_xticks(range(len(categories)))\n",
    "axes[0, 1].set_xticklabels([cat.replace('_', '\\n') for cat in categories], rotation=0, ha='center', fontsize=8)\n",
    "axes[0, 1].axhline(0.7, color='red', linestyle='--', label='Pass Threshold')\n",
    "axes[0, 1].set_ylabel('Average Score')\n",
    "axes[0, 1].set_title('Performance by Category')\n",
    "axes[0, 1].set_ylim(0, 1.0)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Key Metrics Comparison\n",
    "metric_names = ['Tool\\nSelection', 'Goal\\nCompletion', 'Pass\\nRate', 'Idempotency']\n",
    "metric_values = [\n",
    "    metrics['tool_selection_accuracy'],\n",
    "    metrics['goal_completion_rate'],\n",
    "    metrics['pass_rate'],\n",
    "    metrics['idempotency_compliance']\n",
    "]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "axes[1, 0].bar(metric_names, metric_values, color=colors, edgecolor='black')\n",
    "axes[1, 0].set_ylabel('Rate')\n",
    "axes[1, 0].set_title('Key Metrics')\n",
    "axes[1, 0].set_ylim(0, 1.0)\n",
    "for i, v in enumerate(metric_values):\n",
    "    axes[1, 0].text(i, v + 0.03, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Pass/Fail by Difficulty\n",
    "difficulties = ['easy', 'medium', 'hard']\n",
    "diff_data = {diff: {'pass': 0, 'fail': 0} for diff in difficulties}\n",
    "for e in evaluations:\n",
    "    diff = e.difficulty\n",
    "    if e.judge_evaluation.score >= 0.7:\n",
    "        diff_data[diff]['pass'] += 1\n",
    "    else:\n",
    "        diff_data[diff]['fail'] += 1\n",
    "\n",
    "pass_counts = [diff_data[d]['pass'] for d in difficulties]\n",
    "fail_counts = [diff_data[d]['fail'] for d in difficulties]\n",
    "\n",
    "x = np.arange(len(difficulties))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x - width/2, pass_counts, width, label='Pass', color='lightgreen', edgecolor='black')\n",
    "axes[1, 1].bar(x + width/2, fail_counts, width, label='Fail', color='lightcoral', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Difficulty')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Pass/Fail by Difficulty')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels([d.capitalize() for d in difficulties])\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/evaluation/evaluation_results.png', dpi=150, bbox_inches='tight')\n",
    "print(\"‚úÖ Visualization saved to: ../data/evaluation/evaluation_results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16085dc6",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "This evaluation pipeline demonstrates:\n",
    "\n",
    "1. **Golden Dataset**: 10 curated test cases covering all agent capabilities\n",
    "2. **LLM-as-a-Judge**: Gemini evaluates responses based on structured criteria\n",
    "3. **Behavioral Evaluation**: Trajectory analysis (tool selection) alongside goal completion\n",
    "4. **Tracking**: Results saved to CSV for longitudinal monitoring\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Tool Selection Accuracy** shows how well the agent understands intent\n",
    "- **Goal Completion Rate** measures task success\n",
    "- **Idempotency Compliance** ensures safety in production\n",
    "- **Category Breakdown** identifies strengths and weaknesses\n",
    "\n",
    "### Production Recommendations\n",
    "\n",
    "1. **Automated CI/CD**: Run this evaluation on every deployment\n",
    "2. **Threshold Enforcement**: Require 80%+ pass rate before production\n",
    "3. **Continuous Monitoring**: Track metrics over time\n",
    "4. **Expand Dataset**: Add more edge cases as issues are discovered\n",
    "\n",
    "---\n",
    "\n",
    "**Milestone 7 Complete** ‚úÖ\n",
    "\n",
    "References:\n",
    "- Intro to Agents p.29: \"Goal Completion Rate\"\n",
    "- Prototype to Production p.12: \"Golden Dataset & Trajectory Evaluation\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
